<!DOCTYPE html>
<!-- saved from url=(0031)https://ssebs.com/hometodo.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>TODO</title>
	<link rel="stylesheet" type="text/css" href="./hometodo_files/hometodo.css"> </head>

<body>
	<div class="bar">Home</div>
	<div></div>
	<section>
		<div class="left">
			<ul>
				<h3>Priorities</h3>
				<li>Work</li>
				<li>Hobbies</li>
			</ul>
		</div>
	</section>
	<section>
		<div class="left">
			<ul>
				<h3>To-Do</h3>
				<li>VFIO</li>
				<li>Dad stuff</li>
				<li>PeopleDB hosted at home with 4 frontends</li>
				<li>
					Uses <a href="https://github.com/ssebs/flask-ppl-api">flask-ppl-api</a> &amp;:
					<ul>
						<li>React</li>
						<li>React+Redux</li>
						<li>Angular</li>
						<li>Vue</li>
					</ul>
				</li>
			</ul>
		</div>
	</section>
	<div style="clear:both;"></div>
	<hr style="margin-top:40px">
	<section class="section-75">
<h3 style="text-align:center">AWS Engineer</h3>
<p><strong>Introduction</strong></p>
<p>So many people struggle with where to get started with AWS and cloud technologies in general. There is popular "<a href="https://www.reddit.com/r/linuxadmin/comments/2s924h/how_did_you_get_your_start/cnnw1ma/" rel="nofollow">How do I learn to be a Linux admin?</a>" post inspired me to write an equivalent for cloud technologies. This post serves as a guide of goals to grow from basic AWS knowledge to understanding and deploying complex architectures in an automated way. Feel free to pick up where you feel relevant based on prior experience.</p>
<p>Assumptions:</p>
<ul>
<li>You have basic-to-moderate Linux systems administration skills</li>
<li>You are at least familiar with programming/scripting. You don't need to be a whiz but you should have some decent hands-on experience automating and programming.</li>
<li>You are willing to dedicate the time to overcome complex issues.</li>
<li>You have an&nbsp;<a href="https://portal.aws.amazon.com/billing/signup#/start" rel="nofollow">AWS Account</a>&nbsp;and a marginal amount of money to spend improving your skills.</li>
</ul>
<p>How to use this guide:</p>
<ul>
<li>This is not a step by step how-to guide.</li>
<li>You should take each goal and "figure it out". I have hints to guide you in the right direction.</li>
<li>Google is your friend.&nbsp;<a href="https://aws.amazon.com/documentation/" rel="nofollow">AWS Documentation</a>&nbsp;is your friend. Stack Overflow is your friend.</li>
<li>Find out and implement the "right way", not the quick way. Ok, maybe do the quick way first then refactor to the right way before moving on.</li>
<li>Shut down or de-provision as much as you can between learning sessions. You should be able to do everything in this guide for literally less than $50 using the&nbsp;<a href="https://aws.amazon.com/free/" rel="nofollow">AWS Free Tier</a>. Rebuilding often will re-inforce concepts anyway.</li>
<li>Skip ahead and read the&nbsp;<em>Cost Analysis</em>&nbsp;and&nbsp;<em>Automation</em>sections and have them in the back of your mind work through the goals.</li>
<li>Lastly, just get hands on, no better time to start then NOW.</li>
</ul>
<p><strong>Project Overview</strong></p>
<p>This is NOT a guide on how to develop websites on AWS. This uses a website as an excuse to use all the technologies AWS puts at your fingertips. The concepts you will learn going through these exercises apply all over AWS.</p>
<p>This guide takes you through a maturity process from the most basic webpage to an extremely cheap scalable web application. The small app you will build does not matter. It can do anything you want, just keep it simple.</p>
<p>Need an idea? Here: Fortune-of-the-Day - Display a random fortune each page load, have a box at the bottom and a submit button to add a new fortune to the random fortune list.</p>
<p><strong>Account Basics</strong></p>
<ul>
<li>Create an IAM user for your personal use.</li>
<li>Set up MFA for your root user, turn off all root user API keys.</li>
<li>Set up Billing Alerts for anything over a few dollars.</li>
<li>Configure the AWS CLI for your user using API credentials.</li>
<li><em>Checkpoint: You use the AWS CLI to interrogate information about your AWS account.</em></li>
</ul>
<p><strong>Web Hosting Basics</strong></p>
<ul>
<li>Deploy a EC2 VM and host a simple static "Fortune-of-the-Day Coming Soon" web page.</li>
<li>Take a snapshot of your VM, delete the VM, and deploy a new one from the snapshot. Basically disk backup + disk restore.</li>
<li><em>Checkpoint: You can view a simple HTML page served from your EC2 instance.</em></li>
</ul>
<p><strong>Auto Scaling</strong></p>
<ul>
<li>Create an AMI from that VM and put it in an autoscaling group so one VM always exists.</li>
<li>Put a Elastic Load Balancer infront of that VM and load balance between two Availability Zones (one EC2 in each AZ).</li>
<li><em>Checkpoint: You can view a simple HTML page served from both of your EC2 instances. You can turn one off and your website is still accessible.</em></li>
</ul>
<p><strong>External Data</strong></p>
<ul>
<li>Create a DynamoDB table and experiment with loading and retrieving data manually, then do the same via a script on your local machine.</li>
<li>Refactor your static page into your Fortune-of-the-Day website (Node, PHP, Python, whatever) which reads/updates a list of fortunes in the AWS DynamoDB table. (Hint: EC2 Instance Role)</li>
<li><em>Checkpoint: Your HA/AutoScaled website can now load/save data to a database between users and sessions</em></li>
</ul>
<p><strong>Web Hosting Platform-as-a-Service</strong></p>
<ul>
<li>Retire that simple website and re-deploy it on Elastic Beanstalk.</li>
<li>Create a S3 Static Website Bucket, upload some sample static pages/files/images. Add those assets to your Elastic Beanstalk website.</li>
<li>Register a domain (or re-use and existing one). Set Route53 as the Nameservers and use Route53 for DNS. Make&nbsp;<a href="http://www.yourdomain.com/" rel="nofollow">www.yourdomain.com</a>&nbsp;go to your Elastic Beanstalk. Make static.yourdomain.com serve data from the S3 bucket.</li>
<li>Enable SSL for your Static S3 Website. This isn't exactly trivial. (Hint: CloudFront + ACM)</li>
<li>Enable SSL for your Elastic Beanstalk Website.</li>
<li><em>Checkpoint: Your HA/AutoScaled website now serves all data over HTTPS. The same as before, except you don't have to manage the servers, web server software, website deployment, or the load balancer.</em></li>
</ul>
<p><strong>Microservices</strong></p>
<ul>
<li>Refactor your EB website into ONLY providing an API. It should only have a POST/GET to update/retrieve that specific data from DynamoDB. Bonus: Make it a simple REST API. Get rid of&nbsp;<a href="http://www.yourdomain.com/" rel="nofollow">www.yourdomain.com</a>&nbsp;and serve this EB as api.yourdomain.com</li>
<li>Move most of the UI piece of your EB website into your Static S3 Website and use Javascript/whatever to retrieve the data from your api.yourdomain.com URL on page load. Send data to the EB URL to have it update the DynamoDB. Get rid of static.yourdomain.com and change your S3 bucket to serve from&nbsp;<a href="http://www.yourdomain.com/" rel="nofollow">www.yourdomain.com</a>.</li>
<li><em>Checkpoint: Your EB deployment is now only a structured way to retrieve data from your database. All of your UI and application logic is served from the S3 Bucket (via CloudFront). You can support many more users since you're no longer using expensive servers to serve your website's static data.</em></li>
</ul>
<p><strong>Serverless</strong></p>
<ul>
<li>Write a AWS Lambda function to email you a list of all of the Fortunes in the DynamoDB table every night. Implement Least Privilege security for the Lambda Role. (Hint: Lambda using Python 3, Boto3, Amazon SES, scheduled with CloudWatch)</li>
<li>Refactor the above app into a Serverless app. This is where it get's a little more abstract and you'll have to do a lot of research, experimentation on your own.
<ul>
<li>The architecture: Static S3 Website Front-End calls API Gateway which executes a Lambda Function which reads/updates data in the DyanmoDB table.</li>
<li>Use your SSL enabled bucket as the primary domain landing page with static content.</li>
<li>Create an AWS API Gateway, use it to forward HTTP requests to an AWS Lambda function that queries the same data from DynamoDB as your EB Microservice.</li>
<li>Your S3 static content should make Javascript calls to the API Gateway and then update the page with the retrieved data.</li>
<li>Once you have the "Get Fortune" API Gateway + Lambda working, do the "New Fortune" API.</li>
</ul>
</li>
<li><em>Checkpoint: Your API Gateway and S3 Bucket are fronted by CloudFront with SSL. You have no EC2 instances deployed. All work is done by AWS services and billed as consumed.</em></li>
</ul>
<p><strong>Cost Analysis</strong></p>
<ul>
<li>Explore the AWS pricing models and see how much it would .</li>
<li>Answer the following for each of the main architectures you built:
<ul>
<li>Roughly how much would this have costed for a month?</li>
<li>How would I scale this architecture and how would my costs change?</li>
</ul>
</li>
<li>Architectures
<ul>
<li>HA EC2 Instances Serving Static Web Page behind ELB</li>
<li>Elastic Beanstalk SSL Website for only API + DynamoDB Table + Route53 + CloudFront SSL + S3 Static Website everything else</li>
<li>Serverless Website using API Gateway + Lambda Functions + DynamoDB + Route53 + CloudFront SSL + S3 Static Website</li>
</ul>
</li>
</ul>
<p><strong>Automation</strong></p>
<p>!!! This is REALLY important !!!</p>
<ul>
<li>These technologies are the most powerful when they're automated. You can make a Development environment in minutes and experiment and throw it away without a thought. This stuff isn't easy, but it's where the really skilled people excel.</li>
<li>Automate the deployment of the architectures above. Use whatever tool you want. The popular ones are AWS CloudFormation or Teraform. Store your code in CodeCommit. Yes, you can automate the deployment of ALL of the above with native AWS tools.</li>
<li>I suggest when you get each app-related section of the done by hand you go back and automate the provisioning of the infrastructure. For example, automate the provisioning of your EC2 instance. Automate the creation of your S3 Bucket with Static Website Hosting enabled, etc. This is not easy, but it is very rewarding when you see it work.</li>
</ul>
<p><strong>Continuous Delivery</strong></p>
<ul>
<li>As you become more familiar with Automating deployments you should explorer and implement a Continuous Delivery pipeline.</li>
<li>Develop a CI/CD pipeline to automatically update a dev deployment of your infrastructure when new code is published, and then build a workflow to update the production version if approved. Travis CI is a decent SaaS too, Jenkins has a huge following too, if you want to stick with AWS-specific technologies you'll be looking for CodePipeline.</li>
</ul>
<p><strong>Miscellaneous / Bonus</strong></p>
<p>These didn't fit in nicely anywhere but are important AWS topics you should also explore:</p>
<ul>
<li>IAM: You should really learn how to create complex IAM Policies. You would have had to do this for the EC2 Instance Role and Lambda Execution Role, but there are many advanced features.</li>
<li>Networking: Create a new VPC from scratch with multiple subnets (you'll learn a LOT of networking), once that is working create another VPC and peer them together. Get a VM in each subnet to talk to eachother using only their private IP addresses.</li>
<li>KMS: Go back and re-do the early EC2 instance goals but enable encryption on the disk volumes. Learn how to encrypt an AMI.</li>
</ul>
<p><strong>Final Thoughts</strong></p>
<p>I've been recently recruiting for Cloud Systems Engineers and Cloud Systems Administrators. We've interviewed over a dozen local people with relevant resume experience. Every single person we interviewed would probably struggle starting with the DynamoDB/AutoScaling work. I'm finding there are very few people that HAVE ACTUALLY DONE THIS STUFF. Many people are familiar with the concepts, but when pushed for details they don't have answers or admit to just peripheral knowledge. You learn SO MUCH by doing.</p>
<p>If you can't find an excuse or get support to do this as part of your job I would find a small but flashy/impressive personal project that you can build and show off as proof of your skills. Open source it on GitHub, make professional documentation, comment as much as is reasonable, and host a demo of the website. Add links to your LinkedIn, reference it on your resume, work it into interview answers, etc. When in a job interview you'll be able to answer all kinds of real-world questions because you've been-there-done-that with most of AWS' major services.</p>
<p>I'm happy to hear any feedback. I'm considering making THIS post my flashy/impressive personal project in the form of a GitHub repo with sample code for each step, architecture diagrams, etc.</p>


<h3>Linux Sysadmin</h3>
<p>This is what I tell people to do, who ask me “how do I learn to be a Linux sysadmin?”.</p>

<p>1) Set up a KVM hypervisor.  </p>

<p>2) Inside of that KVM hypervisor, install a Spacewalk server. Use CentOS 6 as the distro for all work below. (For bonus points, set up errata importation on the CentOS channels, so you can properly see security update advisory information.)  </p>

<p>3) Create a VM to provide named and dhcpd service to your entire environment. Set up the dhcp daemon to use the Spacewalk server as the pxeboot machine (thus allowing you to use Cobbler to do unattended OS installs).  Make sure that every forward zone you create has a reverse zone associated with it. Use something like “internal.virtnet” (but <em>not</em> “.local”) as your internal DNS zone.  </p>

<p>4) Use that Spacewalk server to automatically (without touching it) install a new pair of OS instances, with which you will then create a Master/Master pair of LDAP servers.  Make sure they register with the Spacewalk server. Do <em>not</em> allow anonymous bind, do <em>not</em> use unencrypted LDAP.  </p>

<p>5) Reconfigure all 3 servers to use LDAP authentication.    </p>

<p>6) Create two new VMs, again unattendedly, which will then be Postgresql VMs. Use pgpool-II to set up master/master replication between them.  Export the database from your Spacewalk server and import it into the new pgsql cluster.  Reconfigure your Spacewalk instance to run off of that server.    </p>

<p>7) Set up a Puppet Master.  Plug it into the Spacewalk server for identifying the inventory it will need to work with.  (Cheat and use ansible for deployment purposes, again plugging into the Spacewalk server.)    </p>

<p>8) Deploy another VM. Install iscsitgt and nfs-kernel-server on it. Export a LUN and an NFS share.  </p>

<p>9) Deploy another VM. Install bakula on it, using the postgresql cluster to store its database.  Register each machine on it, storing to flatfile. Store the bakula VM’s image on the iscsi LUN, and every other machine on the NFS share.  </p>

<p>10) Deploy two more VMs. These will have httpd (Apache2) on them. Leave essentially default for now.  </p>

<p>11) Deploy two <em>more</em> VMs. These will have tomcat on them. Use JBoss Cache to replicate the session caches between them. Use the httpd servers as the frontends for this.  The application you will run is <a href="http://jbosswiki.jboss.org/">JBoss Wiki</a>.  </p>

<p>12) You guessed right, deploy another VM. This will do iptables-based NAT/round-robin loadbalancing between the two httpd servers.   </p>

<p>13) Deploy another VM.  On this VM, install postfix. Set it up to use a gmail account to allow you to have it send emails, and receive messages only from your internal network.  </p>

<p>14) Deploy another VM.  On this VM, set up a Nagios server. Have it use snmp to monitor the communication state of every relevant service involved above. This means doing a “is the right port open” check, <em>and</em> a “I got the right kind of response” check <em>and</em> “We still have filesystem space free” check.  </p>

<p>15) Deploy another VM.  On this VM, set up a syslog daemon to listen to every other server’s input. Reconfigure each other server to send their logging output to various files <em>on the syslog server</em>.  (For extra credit, set up logstash or kibana or greylog to parse those logs.)  </p>

<p>16) Document every last step you did in getting to this point in your brand new Wiki.  </p>

<p>17) Now go back and create Puppet Manifests to ensure that every last one of these machines is authenticating to the LDAP servers, registered to the Spacewalk server, and backed up by the bakula server.  </p>

<p>18) Now go back, reference your documents, and set up a Puppet Razor profile that hooks into each of these things to allow you to recreate, from scratch, each individual server.  </p>

<p>19) Destroy every secondary machine you’ve created and use the above profile to recreate them, joining them to the clusters as needed.  </p>

<p>20) Bonus exercise: create three more VMs. A CentOS 5, 6, and 7 machine.  On <em>each</em> of these machines, set them up to allow you to create custom RPMs and import them into the Spacewalk server instance. Ensure your Puppet configurations work for all three and produce like-for-like behaviors.</p>

<p>Do these things and you will be fully exposed to every aspect of Linux Enterprise systems administration.  Do them well and you will have the <em>technical</em> expertise required to seek “Senior” roles.  If you go whole-hog crash-course full-time it with no other means of income, I would expect it would take between 3 and 6 months to go from “I think I’m good with computers” to achieving all of these – assuming you’re not afraid of IRC and google (and have neither friends nor family …).</p>
	</section>



</body></html>